{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0a8a2-36cb-4476-b8b4-6ff1a9601374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from pathlib import Path \n",
    "from time import strftime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a04df-ecb6-4e01-95cb-9cab0da9e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train_all, y_train_all), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_valid, y_valid = X_train_all[-5000:], y_train_all[-5000:]\n",
    "X_train, y_train = X_train_all[:-5000], y_train_all[:-5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cb344-38bb-4e10-b5fc-25096d041061",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train / 255.)\n",
    "X_valid = (X_valid / 255.) \n",
    "X_test = (X_test / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c745db-a855-42de-b16e-ea305e3da0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The manual tuning algorithm will randomly pick from these values for each hyperparameter\n",
    "\n",
    "param_dict = {\n",
    "    'learning_rate' : [0.0001, 0.0005, 0.001, 0.005, 0.01], \n",
    "    'batch_size' : [16, 32, 64, 128], \n",
    "    'epochs' : [50, 100, 150, 200], \n",
    "    'num_layers' : [2, 4, 6, 8, 10],\n",
    "    'neurons' : [200, 400, 600],\n",
    "    'optimizer' : ['adam', 'sgd'],\n",
    "    'lr_sched' : ['exp', 'poly']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413e904-ed73-41d3-8b72-2c4b65eb3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the function for the random model\n",
    "def random_model(neurons=128, num_layers=2):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=[32, 32, 3]))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73c5a3-bc8f-445a-9056-c8c2dc5ccec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_model(neurons):\n",
    "    # This was a test model used to plot tensorboard learning curves\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=[32, 32, 3]))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d8e80-3a1f-4752-8131-c041a871e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is NOT the random tuning model, it was used to plot the\n",
    "# learning curves from other_model\n",
    "\n",
    "results2 = []\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "\n",
    "    lr_og = 0.0005\n",
    "    \n",
    "    end_learning_rate = 0.01\n",
    "    decay_steps = 10000\n",
    "    lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "        lr_og,\n",
    "        decay_steps,\n",
    "        end_learning_rate,\n",
    "        power=0.5)\n",
    "    \n",
    "    model = other_model(neurons=600) # creating the  model\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    run_logdir = Path(\"my_logs/manual2\") / f\"trial_{trial}\"\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=100)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=200,\n",
    "        validation_data=(X_valid, y_valid), \n",
    "        callbacks=[tensorboard_cb, early_stopping_cb],\n",
    "        batch_size=128\n",
    "    )\n",
    "    \n",
    "    results2.append({\n",
    "        'final_val_acc' : max(history.history['val_accuracy']),\n",
    "        'final_train_acc' : max(history.history['accuracy']),\n",
    "        'run_id' : trial\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73b4fd-32ec-4922-a499-403dab817d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 # printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eacd78-f208-4946-9f59-3d805bf6bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams(param_dict):\n",
    "    # randomly picking values for hyperparameters\n",
    "    list = []\n",
    "    for key, values in param_dict.items():\n",
    "        dict = {key : random.choice(values) for key, values in param_dict.items()}\n",
    "        return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc8f79-36d2-4325-b258-0e3579ad9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(root_logdir=\"my_logs\"):\n",
    "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18753832-3228-499e-b13a-c10af3c2243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the random hyperparameter tuning model\n",
    "results = []\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    hps = get_hyperparams(param_dict)\n",
    "    print(f\"Hyperparams for trial {trial}: {hps}\")\n",
    "\n",
    "    lr_og = hps[\"learning_rate\"] # original learning rate for lr schedulers\n",
    "    lr_sched = hps[\"lr_sched\"] # picking a random lr scheduler\n",
    "    \n",
    "    if lr_sched == \"exp\": \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            lr_og,\n",
    "            decay_steps=100000,\n",
    "            decay_rate=0.96,\n",
    "            staircase=True)\n",
    "    else:\n",
    "        end_learning_rate = 0.01\n",
    "        decay_steps = 10000\n",
    "        lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "            lr_og,\n",
    "            decay_steps,\n",
    "            end_learning_rate,\n",
    "            power=0.5)\n",
    "    \n",
    "    model = random_model(neurons=hps['neurons'], num_layers=hps[\"num_layers\"]) # creating the random model\n",
    "\n",
    "    opt = hps[\"optimizer\"] # picking the random optimizer\n",
    "    if opt == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    run_logdir = Path(\"my_logs/manual3\") / f\"trial_{trial}\"\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=30)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=hps['epochs'],\n",
    "        validation_data=(X_valid, y_valid), \n",
    "        callbacks=[tensorboard_cb, early_stopping_cb],\n",
    "        batch_size=hps['batch_size'],\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'hyperparams' : hps,\n",
    "        'final_val_acc' : max(history.history['val_accuracy']),\n",
    "        'final_train_acc' : max(history.history['accuracy']),\n",
    "        'run_id' : trial\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d7079-c35d-4271-af4f-cde3aee4ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs/manual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a74348-f135-4611-8205-969d774e0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
