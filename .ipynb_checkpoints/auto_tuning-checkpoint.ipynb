{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdf416-15d5-4924-824c-7d6484e68493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5f736-f012-4dfc-aac6-1e8a707bc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train_all, y_train_all), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_valid, y_valid = X_train_all[-5000:], y_train_all[-5000:]\n",
    "X_train, y_train = X_train_all[:-5000], y_train_all[:-5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4493243-980d-42e8-a3e7-182464aa6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train / 255.)\n",
    "X_valid = (X_valid / 255.) \n",
    "X_test = (X_test / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf54e7c-73d0-4bec-b333-7efe78676374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    '''This is the model with a learning rate scheduler\n",
    "    and an optimizer choice which was used in the project'''\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=2, max_value=10, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=200, max_value=800)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, \n",
    "                                sampling=\"log\")\n",
    "\n",
    "    lr_sched = hp.Choice(\"lr_scheduler\", values=[\"exp\", \"poly\"]) # ExponentialDecay and PolynomialDecay\n",
    "    if lr_sched == \"exp\": \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            learning_rate,\n",
    "            decay_steps=100000,\n",
    "            decay_rate=0.96,\n",
    "            staircase=True)\n",
    "    else:\n",
    "        end_learning_rate = 0.01\n",
    "        decay_steps = 10000\n",
    "        lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "            learning_rate,\n",
    "            decay_steps,\n",
    "            end_learning_rate,\n",
    "            power=0.5)\n",
    "    \n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"relu\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f82804-25ab-49f6-afe2-dffd1ce6f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    '''This model does not have the learning rate scheduler \n",
    "    choice and was only used to test KerasTuner. None of the results\n",
    "    from this model were included in the final report'''\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=2, max_value=10, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=200, max_value=800)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, \n",
    "                                sampling=\"log\")\n",
    "    \n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"relu\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef3060-9be8-4c0c-8153-d81a36c958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from time import strftime \n",
    "\n",
    "def get_run_logdir(root_logdir=\"my_logs\"):\n",
    "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c977997-07db-4ef7-96a4-5ff1a699d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a8400-0ef2-4aae-a2c5-90952f285ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperband Tuner\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
    "    hyperband_iterations=2, overwrite=True, directory=\"my_logs\", project_name=\"keras_tn\")\n",
    "\n",
    "root_logdir = Path(tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "                 callbacks=[early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5b28c-299d-46c5-a545-2dc6743d64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianOptimization Tuner\n",
    "\n",
    "folder = 'bay2/tensorboard'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "bayesian_tuner = kt.BayesianOptimization(\n",
    "    MyClassificationHyperModel(), max_trials=30, alpha=1e-4, beta=2.6, objective=\"val_accuracy\",\n",
    "    overwrite=True, directory=\"bay2\", project_name=\"bay2\")\n",
    "\n",
    "root_logdir2 = Path(bayesian_tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb2 = tf.keras.callbacks.TensorBoard(root_logdir2)\n",
    "early_stopping_cb2 = tf.keras.callbacks.EarlyStopping(patience=100)\n",
    "bayesian_tuner.search(X_train, y_train, validation_data=(X_valid, y_valid), epochs=200,\n",
    "                 callbacks=[early_stopping_cb2, tensorboard_cb2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ba0f3-638e-4b3c-b281-9dc1b17d654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./keras_tn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
