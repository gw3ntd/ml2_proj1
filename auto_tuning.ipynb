{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebdf416-15d5-4924-824c-7d6484e68493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 15:27:57.741443: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-05 15:27:57.750103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-05 15:27:57.760537: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-05 15:27:57.763681: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-05 15:27:57.771603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f5f736-f012-4dfc-aac6-1e8a707bc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train_all, y_train_all), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_valid, y_valid = X_train_all[-5000:], y_train_all[-5000:]\n",
    "X_train, y_train = X_train_all[:-5000], y_train_all[:-5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4493243-980d-42e8-a3e7-182464aa6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train / 255.)\n",
    "X_valid = (X_valid / 255.) \n",
    "X_test = (X_test / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf54e7c-73d0-4bec-b333-7efe78676374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=2, max_value=10, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=200, max_value=800)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, \n",
    "                                sampling=\"log\")\n",
    "\n",
    "    lr_sched = hp.Choice(\"lr_scheduler\", values=[\"exp\", \"poly\"])\n",
    "    if lr_sched == \"exp\": \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            learning_rate,\n",
    "            decay_steps=100000,\n",
    "            decay_rate=0.96,\n",
    "            staircase=True)\n",
    "    else:\n",
    "        end_learning_rate = 0.01\n",
    "        decay_steps = 10000\n",
    "        lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "            learning_rate,\n",
    "            decay_steps,\n",
    "            end_learning_rate,\n",
    "            power=0.5)\n",
    "    \n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"relu\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f82804-25ab-49f6-afe2-dffd1ce6f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=2, max_value=10, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=200, max_value=800)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, \n",
    "                                sampling=\"log\")\n",
    "    \n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"relu\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc250be5-3493-4030-8609-da4ddf925e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
    "    directory=\"my_logs\", project_name=\"keras_tn\", seed=42)\n",
    "random_search_tuner.search(X_train, y_train, epochs=10, \n",
    "                               validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da288b-6c3b-475c-a96f-2cea0e55912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932e4ef-183f-4783-b61c-cd55468cd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
    "top3_params[0].values # best hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e858eb-7010-4d24-af3e-1dfacd36d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0] \n",
    "best_trial.summary() # summary of best trial\n",
    "best_trial.metrics.get_last_value(\"val_accuracy\") # best hyperparams + validation acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb55a7-deba-4d94-b0b4-13fa79f5c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "'''\n",
    "If model performance is good, I can continue training it for a few epochs on the training set\n",
    "(X_train_full, y_train_full), then evaluate it on the test set, and deploy it to production\n",
    "'''\n",
    "best_model.fit(X_train_full, y_train_full, epochs=10)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef3060-9be8-4c0c-8153-d81a36c958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from time import strftime \n",
    "\n",
    "def get_run_logdir(root_logdir=\"my_logs\"):\n",
    "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f15043-8960-414e-8136-bc2994e2d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c977997-07db-4ef7-96a4-5ff1a699d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a8400-0ef2-4aae-a2c5-90952f285ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
    "    hyperband_iterations=2, overwrite=True, directory=\"my_logs\", project_name=\"keras_tn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61cff0-2ee9-43d6-b357-c1d0806336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run\n",
    "\n",
    "root_logdir = Path(tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "                 callbacks=[early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c5b28c-299d-46c5-a545-2dc6743d64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 03m 56s]\n",
      "val_accuracy: 0.09860000014305115\n",
      "\n",
      "Best val_accuracy So Far: 0.15119999647140503\n",
      "Total elapsed time: 01h 50m 31s\n"
     ]
    }
   ],
   "source": [
    "folder = 'bay2/tensorboard'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "bayesian_tuner = kt.BayesianOptimization(\n",
    "    MyClassificationHyperModel(), max_trials=30, alpha=1e-4, beta=2.6, objective=\"val_accuracy\",\n",
    "    overwrite=True, directory=\"bay2\", project_name=\"bay2\")\n",
    "\n",
    "root_logdir2 = Path(bayesian_tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb2 = tf.keras.callbacks.TensorBoard(root_logdir2)\n",
    "early_stopping_cb2 = tf.keras.callbacks.EarlyStopping(patience=100)\n",
    "bayesian_tuner.search(X_train, y_train, validation_data=(X_valid, y_valid), epochs=200,\n",
    "                 callbacks=[early_stopping_cb2, tensorboard_cb2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
